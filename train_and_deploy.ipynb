{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Title\n",
    "\n",
    "This notebook lists all the steps that you need to complete the complete this project. You will need to complete all the TODOs in this notebook as well as in the README and the two python scripts included with the starter code.\n",
    "\n",
    "\n",
    "**TODO**: Give a helpful introduction to what this notebook is for. Remember that comments, explanations and good documentation make your project informative and professional.\n",
    "\n",
    "**Note:** This notebook has a bunch of code and markdown cells with TODOs that you have to complete. These are meant to be helpful guidelines for you to finish your project while meeting the requirements in the project rubrics. Feel free to change the order of these the TODO's and use more than one TODO code cell to do all your tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smdebug in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.12)\n",
      "Requirement already satisfied: boto3>=1.10.32 in c:\\programdata\\anaconda3\\lib\\site-packages (from smdebug) (1.26.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from smdebug) (4.21.9)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from smdebug) (1.19.2)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from smdebug) (20.4)\n",
      "Requirement already satisfied: pyinstrument==3.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from smdebug) (3.4.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3>=1.10.32->smdebug) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3>=1.10.32->smdebug) (1.29.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3>=1.10.32->smdebug) (0.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->smdebug) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->smdebug) (1.15.0)\n",
      "Requirement already satisfied: pyinstrument-cext>=0.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyinstrument==3.4.2->smdebug) (0.2.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.30.0,>=1.29.0->boto3>=1.10.32->smdebug) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.30.0,>=1.29.0->boto3>=1.10.32->smdebug) (1.25.11)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.116.0.tar.gz (592 kB)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in c:\\programdata\\anaconda3\\lib\\site-packages (from sagemaker) (1.26.0)\n",
      "Collecting google-pasta\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sagemaker) (1.19.2)\n",
      "Collecting protobuf<4.0,>=3.1\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-win_amd64.whl (904 kB)\n",
      "Collecting protobuf3-to-dict<1.0,>=0.1.5\n",
      "  Using cached protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "Collecting smdebug_rulesconfig==1.0.1\n",
      "  Using cached smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sagemaker) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sagemaker) (20.4)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from sagemaker) (1.1.3)\n",
      "Collecting pathos\n",
      "  Using cached pathos-0.3.0-py3-none-any.whl (79 kB)\n",
      "Collecting schema\n",
      "  Using cached schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.29.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from google-pasta->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->sagemaker) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->sagemaker) (2.8.1)\n",
      "Collecting pox>=0.3.2\n",
      "  Using cached pox-0.3.2-py3-none-any.whl (29 kB)\n",
      "Collecting dill>=0.3.6\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting ppft>=1.7.6.6\n",
      "  Using cached ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
      "Collecting multiprocess>=0.70.14\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from schema->sagemaker) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.30.0,>=1.29.0->boto3<2.0,>=1.20.21->sagemaker) (1.25.11)\n",
      "Building wheels for collected packages: sagemaker, protobuf3-to-dict\n",
      "  Building wheel for sagemaker (setup.py): started\n",
      "  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.116.0-py2.py3-none-any.whl size=809057 sha256=9399821bc53760e275c3fb06148ea71d0eb1670949d11f3b452479e866221dbf\n",
      "  Stored in directory: c:\\users\\bbdnet2184\\appdata\\local\\pip\\cache\\wheels\\3e\\cb\\b1\\5b13ff7b150aa151e4a11030a6c41b1e457c31a52ea1ef11b0\n",
      "  Building wheel for protobuf3-to-dict (setup.py): started\n",
      "  Building wheel for protobuf3-to-dict (setup.py): finished with status 'done'\n",
      "  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4033 sha256=d4947dbc77204322a7140c0af9690c2c192e048b1beefdc4215542af5903c6d5\n",
      "  Stored in directory: c:\\users\\bbdnet2184\\appdata\\local\\pip\\cache\\wheels\\fc\\10\\27\\2d1e23d8b9a9013a83fbb418a0b17b1e6f81c8db8f53b53934\n",
      "Successfully built sagemaker protobuf3-to-dict\n",
      "Installing collected packages: google-pasta, protobuf, protobuf3-to-dict, smdebug-rulesconfig, pox, dill, ppft, multiprocess, pathos, schema, sagemaker\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.9\n",
      "    Uninstalling protobuf-4.21.9:\n",
      "      Successfully uninstalled protobuf-4.21.9\n",
      "Successfully installed dill-0.3.6 google-pasta-0.2.0 multiprocess-0.70.14 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 protobuf-3.20.3 protobuf3-to-dict-0.1.5 sagemaker-2.116.0 schema-0.7.5 smdebug-rulesconfig-1.0.1\n"
     ]
    }
   ],
   "source": [
    "# TODO: Install any packages that you might need\n",
    "# For instance, you will need the smdebug package\n",
    "!pip install smdebug\n",
    "# !pip install sagemaker only needed locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import any packages that you might need\n",
    "# For instance you will need Boto3 and Sagemaker\n",
    "import sagemaker\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name voclabs to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: course3-project\n",
      "Execution role: arn:aws:iam::569113498951:role/voclabs\n",
      "Sagemaker Session: <sagemaker.session.Session object at 0x0000023BA587E160>\n",
      "Sagemaker Session Region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# TODO: Get some common variables\n",
    "# sagemaker session/client\n",
    "# default or specific bucket\n",
    "# execution role\n",
    "# others in course 2 project - check what other common things were done\n",
    "\n",
    "bucket = \"course3-project\"\n",
    "execution_role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Execution role: {execution_role}\")\n",
    "print(f\"Sagemaker Session: {sagemaker_session}\")\n",
    "print(f\"Sagemaker Session Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "TODO: Explain what dataset you are using for this project. Maybe even give a small overview of the classes, class distributions etc that can help anyone not familiar with the dataset get a better understand of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fetch and upload the data to AWS S3\n",
    "# Upload? This was already there using wget - but I decided to use my own data set. Is using the dog data set better than mine?\n",
    "# Comment out below because I have already uploaded cell data to s3 using the Web Console\n",
    "\n",
    "# Command to download and unzip data\n",
    "# !wget https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
    "# !unzip dogImages.zip\n",
    "\n",
    "# Will need to extract data in s3 and upload it in appropriate format to be used by training script\n",
    "# This extract portion of the ETL need not be performed in the Data class of the trianing script\n",
    "# can extract locally and then push to s3 or extract in s3 itself in this notebook or submission script?\n",
    "# what is the submission script?\n",
    "# need to also save the labels on the images\n",
    "# might need to do that here\n",
    "# once of vs recurring data tasks? new images vs old images...?\n",
    "# what should be in training script and what not!!!\n",
    "# I think best done before trinaig script!!!\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def extract_blood_cell_data(path_to_zip_file):\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./data\")\n",
    "        \n",
    "import os\n",
    "# Do we have a validation data set?\n",
    "# Will we be using one?\n",
    "# Local Data Paths\n",
    "train_dir = os.path.join(os.getcwd(), \"data/train\")\n",
    "test_dir = os.path.join(os.getcwd(), \"data/test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "# !mkdir ./data\n",
    "\n",
    "# Data Paths in s3\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket, \"data/dataset2-master.zip\", \"./data/dataset2-master.zip\")\n",
    "extract_blood_cell_data(\"./data/dataset2-master.zip\")\n",
    "\n",
    "# is there a need to create annotated file? - or just append the label to the file name.\n",
    "\n",
    "# 4 Class Directories\n",
    "# Train\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "train_neutrophil=glob.glob(\"./data/dataset2-master/dataset2-master/images/TRAIN/NEUTROPHIL\" + '/**/*.jpeg', recursive=True))\n",
    "train_monocyte=glob.glob(\"./data/dataset2-master/dataset2-master/images/TRAIN/MONOCYTE\" + '/**/*.jpeg', recursive=True))\n",
    "train_lymphocyte=glob.glob(\"./data/dataset2-master/dataset2-master/images/TRAIN/LYMPHOCYTE\" + '/**/*.jpeg', recursive=True))\n",
    "train_eosinophil=glob.glob(\"./data/dataset2-master/dataset2-master/images/TRAIN/EOSINOPHIL\" + '/**/*.jpeg', recursive=True))\n",
    "\n",
    "df = pd.DataFrame(columns = ['ImageFileName', 'Class'])\n",
    "for i in range(len(train_neutrophil)):\n",
    "    pd.append({'ImageFileName': train_neutrophil[i], 'Class': '0'})\n",
    "\n",
    "for i in range(len(train_monocyte)):\n",
    "    pd.append({'ImageFileName': train_monocyte[i], 'Class': '1'})\n",
    "    \n",
    "for i in range(len(train_lymphocyte)):\n",
    "    pd.append({'ImageFileName': train_lymphocyte[i], 'Class': '2'})\n",
    "    \n",
    "for i in range(len(train_eosinophil)):\n",
    "    pd.append({'ImageFileName': train_eosinophil[i], 'Class': '3'})\n",
    "\n",
    "df.to_csv(\"train.lst\", header=False)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"DEFAULT_S3_BUCKET\"] = bucket\n",
    "\n",
    "!aws s3 sync ./data/dataset2-master/dataset2-master/images/TRAIN/NEUTROPHIL s3://${DEFAULT_BUCKET}/data/train\n",
    "!aws s3 sync ./data/dataset2-master/dataset2-master/images/TRAIN/MONOCYTE s3://${DEFAULT_BUCKET}/data/train\n",
    "!aws s3 sync ./data/dataset2-master/dataset2-master/images/TRAIN/LYMPHOCYTE s3://${DEFAULT_BUCKET}/data/train\n",
    "!aws s3 sync ./data/dataset2-master/dataset2-master/images/TRAIN/EOSINOPHIL s3://${DEFAULT_BUCKET}/data/train\n",
    "\n",
    "# separate training script for aws vs local - aws will also fetch the data?\n",
    "# from s3? - not in a public location - my s3 bucket in my account\n",
    "\n",
    "# lst file etc is about how we planning to read the data on our side - which folder it saves it to - the estimator saves it to a folder on the instance\n",
    "# environement variable -parser argument - input channel\n",
    "# when running training job the logs show this!!! the env variable\n",
    "\n",
    "# saving data to s3 in a way that will be easily loadable by the Data class and minimal work for transformers and loaders - which fits the design pattern\n",
    "# numpy like format? maybe even as pickle files with numpy format to use? - any advantage?\n",
    "# or disadvantage to pickle files?\n",
    "# we only need to the image for any data exploration and visualisatiosn which we should ideally not do on the training script? - Check how CIFAR works in PYTORCH - the soruce implementation\n",
    "# since numpy is what most models prefer - since numpy can easily be changed to tensors which our chosen pytorhc framwerk can use?\n",
    "# check tutorial I am using and project 2 to see best ways to do it and course souham videos and idoms\n",
    "# clear disticinction between train and test\n",
    "# generating a dataset\n",
    "# train validation split\n",
    "# seems like it will fetch input channels all of them in the directory? for script mode - for geneirc estimator not sure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "**TODO:** This is the part where you will finetune a pretrained model with hyperparameter tuning. Remember that you have to tune a minimum of two hyperparameters. However you are encouraged to tune more. You are also encouraged to explain why you chose to tune those particular hyperparameters and the ranges.\n",
    "\n",
    "**Note:** You will need to use the `hpo.py` script to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Declare your HP ranges, metrics etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create estimators for your HPs\n",
    "\n",
    "estimator = # TODO: Your estimator here\n",
    "\n",
    "tuner = # TODO: Your HP tuner here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit your HP Tuner\n",
    "tuner.fit() # TODO: Remember to include your data channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the best estimators and the best HPs\n",
    "\n",
    "best_estimator = #TODO\n",
    "\n",
    "#Get the hyperparameters of the best trained model\n",
    "best_estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Profiling and Debugging\n",
    "TODO: Using the best hyperparameters, create and finetune a new model\n",
    "\n",
    "**Note:** You will need to use the `train_model.py` script to perform model profiling and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up debugging and profiling rules and hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and fit an estimator\n",
    "\n",
    "estimator = # TODO: Your estimator here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a debugging output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
    "**TODO**: If not, suppose there was an error. What would that error look like and how would you have fixed it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the profiler output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deploying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Deploy your model to an endpoint\n",
    "\n",
    "predictor=estimator.deploy() # TODO: Add your deployment configuration like instance type and number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run an prediction on the endpoint\n",
    "\n",
    "image = # TODO: Your code to load and preprocess image to send to endpoint for prediction\n",
    "response = predictor.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remember to shutdown/delete your endpoint once your work is done\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
